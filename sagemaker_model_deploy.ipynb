{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session and role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# location of stored sata\n",
    "data_dir = 'modelling'\n",
    "# S3 location\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "# set prefix, a descriptive name for a directory  \n",
    "prefix = 'sagemaker/audio_emotion_detection'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and deploy saved model\n",
    "\n",
    "An AWS blogpost about deployment of a keras model on SageMaker (https://aws.amazon.com/blogs/machine-learning/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker/) gave me a good general idea how to proceed. First, I changed the training approach to save model in a format enabling deployment on SageMaker. The blogpost described a workaround to deploy a model, by using sagemaker.tensorflow.model.TensorFlowModel and simulating its creation with an empty train.py file. As I discovered from the Sagemaker documentation, there is now a better way to deploy directly from model artifacts (https://sagemaker.readthedocs.io/en/stable/using_tf.html#deploying-directly-from-model-artifacts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved_model.pb\tvariables\n"
     ]
    }
   ],
   "source": [
    "!ls export/1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Tar the entire directory and upload to Amazon S3\n",
    "import tarfile\n",
    "with tarfile.open('model.tar.gz', mode='w:gz') as archive:\n",
    "    archive.add('export', recursive=True)\n",
    "\n",
    "inputs = sagemaker_session.upload_data(path='model.tar.gz', key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the trained model\n",
    "\n",
    "TensorFlow Serving Model\n",
    "https://sagemaker.readthedocs.io/en/stable/sagemaker.tensorflow.html\n",
    "\n",
    "Specification of *framework_version='2.0.0'* alighns the TensorFlow version of the trained model with the version of the serving model. Otherwise, an error was thrown.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow.serving import Model\n",
    "model = Model(model_data = 's3://' + bucket + '/' + prefix + '/' + 'model.tar.gz', \n",
    "              role = role,\n",
    "             framework_version='2.0.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------!CPU times: user 645 ms, sys: 22 ms, total: 667 ms\n",
      "Wall time: 7min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predictor = model.deploy(initial_instance_count=1, \n",
    "                         instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tensorflow-inference-2019-12-26-08-24-13-228'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions\n",
    "\n",
    "Sending a preprocessed sound data (MFCC feature) to the endpoint results in probability prediction for each of the classes. The endpoint accepts different formats and an appropriate should be chosen to be used with the lambda function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON-formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc = [-5.5037286e+02,  4.8238766e+01,  1.8442627e+01,  6.5049920e+00,\n",
    "        8.2730311e-01, -6.1545024e+00, -2.4476097e+01, -7.4760280e+00,\n",
    "       -1.1998719e+01, -9.4280348e+00, -7.5956612e+00, -5.5952277e+00,\n",
    "       -4.4645872e+00, -6.2611837e+00, -3.2972498e+00, -8.0563803e+00,\n",
    "       -4.3905830e+00, -2.1417410e+00, -6.5392509e+00, -6.4882522e+00,\n",
    "       -6.0786166e+00, -8.1123333e+00, -5.5041442e+00,  4.6315003e-02,\n",
    "       -1.7572067e+00,  8.9478511e-01, -1.2644883e-01,  2.5937853e+00,\n",
    "       -1.1436307e+00,  3.4961894e+00,  2.2928705e+00,  3.7813089e+00,\n",
    "        4.3201895e+00,  1.7073919e+00,  1.4069341e+00, -1.3996479e-01,\n",
    "       -3.3032024e-01,  1.0882645e+00, -5.8361721e-01,  1.6979592e+00]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = {\n",
    "  'instances': [mfcc]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': [[0.0174345262,\n",
       "   0.0509620309,\n",
       "   0.61150676,\n",
       "   0.00767038902,\n",
       "   0.312426269]]}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = predictor.predict(input)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = [mfcc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': [[0.0174345262,\n",
       "   0.0509620309,\n",
       "   0.61150676,\n",
       "   0.00767038902,\n",
       "   0.312426269]]}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = predictor.predict(input)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSV (comma-separated values) fortmat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': [[0.0174345262,\n",
       "   0.0509620309,\n",
       "   0.61150676,\n",
       "   0.00767038902,\n",
       "   0.312426269]]}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.tensorflow.serving import Predictor\n",
    "# create a Predictor with JSON serialization\n",
    "predictor = Predictor(predictor.endpoint, serializer=sagemaker.predictor.csv_serializer)\n",
    "\n",
    "# CSV-formatted string input\n",
    "input = mfcc\n",
    "\n",
    "result = predictor.predict(input)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result['predictions'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform into a label prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_mapping_dict = {'angry': 0, 'fear': 1, 'happy': 2, 'neutral': 3, 'sad': 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'happy'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the index of a maximum value\n",
    "max_idx = np.argmax(result)\n",
    "# get key of a value equal to the index of a maximum value\n",
    "# sourced from https://stackoverflow.com/questions/8023306/get-key-by-value-in-dictionary\n",
    "result_label = list(class_mapping_dict.keys())[list(class_mapping_dict.values()).index(max_idx)]\n",
    "result_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke endpoint as by a Lambda function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = {'instances': [mfcc]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"instances\": [[-550.37286, 48.238766, 18.442627, 6.504992, 0.82730311, -6.1545024, -24.476097, -7.476028, -11.998719, -9.4280348, -7.5956612, -5.5952277, -4.4645872, -6.2611837, -3.2972498, -8.0563803, -4.390583, -2.141741, -6.5392509, -6.4882522, -6.0786166, -8.1123333, -5.5041442, 0.046315003, -1.7572067, 0.89478511, -0.12644883, 2.5937853, -1.1436307, 3.4961894, 2.2928705, 3.7813089, 4.3201895, 1.7073919, 1.4069341, -0.13996479, -0.33032024, 1.0882645, -0.58361721, 1.6979592]]}'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "json.dumps(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "runtime = boto3.Session().client('sagemaker-runtime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we use the SageMaker runtime to invoke our endpoint, sending the review we were given\n",
    "response = runtime.invoke_endpoint(EndpointName = predictor.endpoint, # The name of the endpoint\n",
    "                                       ContentType = 'application/json', # The data format that is expected\n",
    "                                       Body = json.dumps(input) # The preprocessed audio file\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '3f9b9ef1-1fa8-4191-a7c5-a50f50735380',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '3f9b9ef1-1fa8-4191-a7c5-a50f50735380',\n",
       "   'x-amzn-invoked-production-variant': 'AllTraffic',\n",
       "   'date': 'Thu, 26 Dec 2019 10:27:52 GMT',\n",
       "   'content-type': 'application/json',\n",
       "   'content-length': '98'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ContentType': 'application/json',\n",
       " 'InvokedProductionVariant': 'AllTraffic',\n",
       " 'Body': <botocore.response.StreamingBody at 0x7f9b99821240>}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n    \"predictions\": [[0.0174345262, 0.0509620309, 0.61150676, 0.00767038902, 0.312426269]\\n    ]\\n}'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The response is an HTTP response whose body contains the result of our inference\n",
    "result = response['Body'].read().decode('utf-8')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thansform to json\n",
    "result = json.loads(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract an array of probability predictions\n",
    "result = result['predictions'][0]\n",
    "    \n",
    "# get the index of a maximum value\n",
    "max_idx = np.argmax(result)\n",
    "# get key of a value equal to the index of a maximum value\n",
    "# sourced from https://stackoverflow.com/questions/8023306/get-key-by-value-in-dictionary\n",
    "result_label = list(class_mapping_dict.keys())[list(class_mapping_dict.values()).index(max_idx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'happy'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "local_metadata": {},
    "remote_metadata": {
     "pycharm": {
      "name": "#%%\n"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Accepts a predictor endpoint as input\n",
    "# And deletes the endpoint by name\n",
    "def delete_endpoint(predictor):\n",
    "        try:\n",
    "            boto3.client('sagemaker').delete_endpoint(EndpointName=predictor.endpoint)\n",
    "            print('Deleted {}'.format(predictor.endpoint))\n",
    "        except:\n",
    "            print('Already deleted: {}'.format(predictor.endpoint))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
